# -*- coding: utf-8 -*-
"""FIFA Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11DDufYCYECyMbn2GJYJfKqxiQeR246yk
"""

from google.colab import drive

import pandas as pd

#experimenting with pipelines
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder
from sklearn.compose import ColumnTransformer

from sklearn.model_selection import train_test_split

from sklearn.linear_model import LinearRegression

import numpy as np
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score

from sklearn.tree import DecisionTreeRegressor

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer
import pickle as pkl

import sklearn
print(sklearn.__version__)

drive.mount('/content/drive')

data  = pd.read_csv('/content/drive/MyDrive/Datasets/male_players (legacy).csv')
data_copy = pd.read_csv('/content/drive/MyDrive/Datasets/male_players (legacy).csv')

players_22 = pd.read_csv('/content/drive/MyDrive/Datasets/players_21.csv')

"""Feature engineering"""

df = data
df.drop(columns = ['fifa_update_date', 'club_joined_date', 'player_id', 'fifa_version', 'league_id', 'club_contract_valid_until_year', 'fifa_update'], inplace= True)
#dropping useless columns
y = df['overall']

df.info()

df.describe()

categorical = df.select_dtypes(include=['object'])

quant_df = df.drop(columns = categorical.columns)

quant_df.info()

corr_matrix = quant_df.corr()

corr_matrix['overall'].sort_values(ascending=False)

"""I think the important categorical features that could correlate strongly with the overall rating are body type, work rate and preferred foot"""

important_categorical_features = [column for column in categorical if column in  ['body_type', 'work_rate', 'preferred_foot',]]

non_important = [column for column in categorical.columns if column not in ['body_type',  'work_rate', 'preferred_foot', ]]

for column in quant_df.columns:
  if corr_matrix['overall'][column] < 0.4 and corr_matrix['overall'][column] > -0.4:
    non_important.append(column)
#add the values that don't correlate strongly to non important columns

non_important

cat_pipeline =  OneHotEncoder(handle_unknown = 'ignore', sparse = False) #pipeline to handle categorical data

categorical.drop(columns = [column for column in categorical.columns if column in non_important], inplace = True)
categorical.drop(columns = [column for column in categorical.columns if categorical[column].isnull().sum() > 0.4*categorical.shape[0]], inplace = True)

quant_df.drop(columns = [column for column in quant_df.columns if column in non_important], inplace = True)
quant_df.drop(columns = [column for column in quant_df.columns if quant_df[column].isnull().sum() > 0.4*quant_df.shape[0]], inplace = True)

categorical = cat_pipeline.fit_transform(categorical)
categorical = pd.DataFrame(categorical, columns = cat_pipeline.get_feature_names_out(important_categorical_features))

quant_df.info()

quant_cols = quant_df.columns
imputer = IterativeImputer(max_iter = 10, random_state = 0)
scaler = StandardScaler()
quant_df = imputer.fit_transform(quant_df)
quant_df = scaler.fit_transform(quant_df)
quant_df = pd.DataFrame(quant_df, columns = quant_cols)

quant_df.head()

quant_df.info()

#function for a pipeline
def drop_null_columns(df):
  for column in df.columns:
    if df[column].isnull().sum() > 0.4*df.shape[0]:
      df.drop(columns = [column])
  return df
drop_nulls = FunctionTransformer(drop_null_columns)

#functions for a pipeline
def drop_columns(df, columns):
  return df.drop(columns = columns)
column_dropper = FunctionTransformer(drop_columns, kw_args = {'columns': non_important})

quant = Pipeline([
    ('impute', imputer),
    ('scaler', scaler)
]) #pipeline for quantitative columns

column_transformer = ColumnTransformer(transformers = [
    ('cat_transformer', cat_pipeline, important_categorical_features), #this works on the important categorical features
    ('quant_transformer', quant, [column for column in df.columns if (df[column].dtype == 'float64' or df[column].dtype == 'int64') and column not in non_important])
    #works on quantitative features
]
)

pipeline = Pipeline([
    ('dropping_unimportant', column_dropper),
    ('dropping_null_columns', drop_nulls),
    ('preprocessing', column_transformer),

])
#X = pipeline.fit_transform(df) #transforms the dataframe according to the pipeline

one_hot_features = cat_pipeline.get_feature_names_out(important_categorical_features) #gets the new features from the one hot encoding

new_columns = list(one_hot_features)+ [column for column in df.columns if column not in non_important and column not in important_categorical_features]
new_columns

X = pd.concat([categorical, quant_df], axis =1)

X.info()

X.head()

X.drop('overall', axis = 1, inplace = True)

"""Splitting data into the test data and training data"""

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, y, test_size = 0.2, random_state = 47)

lin_reg_model = LinearRegression()

lin_reg_model.fit(Xtrain, Ytrain)

y_pred_1 = lin_reg_model.predict(Xtest)

print(f"""
Mean Absolute Error = {mean_absolute_error(y_pred_1, Ytest)},
Mean Squared Error = {mean_squared_error(y_pred_1, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(y_pred_1, Ytest))},
R2 score = {r2_score(y_pred_1, Ytest)}
""")

"""Processing the test data (players_22)"""

new_test_X = pipeline.fit_transform(players_22)
new_test_X = pd.DataFrame(new_test_X, columns = new_columns)
new_test_Y = players_22['overall']
new_test_X.drop('overall', axis=1, inplace = True)

new_test_X.head()

a = list(quant_df.columns)
a.pop(0)
new_test_X[a].head()

new_test_pred = lin_reg_model.predict(new_test_X)

print(f"""
Mean Absolute Error = {mean_absolute_error(new_test_pred, new_test_Y)},
Mean Squared Error = {mean_squared_error(new_test_pred, new_test_Y)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(new_test_pred, new_test_Y))},
R2 score = {r2_score(new_test_pred, new_test_Y)}
""")

dTree = DecisionTreeRegressor()

dTree.fit(Xtrain, Ytrain)

tree_pred = dTree.predict(Xtest)

print(f"""
Mean Absolute Error = {mean_absolute_error(tree_pred, Ytest)},
Mean Squared Error = {mean_squared_error(tree_pred, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(tree_pred, Ytest))},
R2 score = {r2_score(tree_pred, Ytest)}
""")

new_tree_pred = dTree.predict(new_test_X)

print(f"""
Mean Absolute Error = {mean_absolute_error(new_tree_pred, new_test_Y)},
Mean Squared Error = {mean_squared_error(new_tree_pred, new_test_Y)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(new_tree_pred, new_test_Y))},
R2 score = {r2_score(new_tree_pred, new_test_Y)}
""")

"""Training an ensemble model. The Random Forest Regressor"""

forest_reg = RandomForestRegressor(n_jobs = -1) #this is an ensemble model

parameters = {
    'n_estimators': [10,50,100],
    'max_depth': [5,10, 15, 20]
}

rand_search = RandomizedSearchCV(forest_reg, parameters, scoring='neg_root_mean_squared_error', cv = 3, n_jobs=-1)
 #grid search helps us figure out the best hyper parameters based on the parameters model
 #but randomized search is faster

"""Training with the training data"""

rand_search.fit(Xtrain, Ytrain)

"""Initial testing"""

rand_pred = rand_search.predict(Xtest)

print(f"""
Mean Absolute Error = {mean_absolute_error(rand_pred, Ytest)},
Mean Squared Error = {mean_squared_error(rand_pred, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(rand_pred, Ytest))},
R2 score = {r2_score(rand_pred, Ytest)}
""")

"""Testing with the players 22 data"""

rand_pred_new = rand_search.predict(new_test_X)

print(f"""
Mean Absolute Error = {mean_absolute_error(rand_pred_new, new_test_Y)},
Mean Squared Error = {mean_squared_error(rand_pred_new, new_test_Y)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(rand_pred_new, new_test_Y))},
R2 score = {r2_score(rand_pred_new, new_test_Y)}
""")

"""Investigating feature importances.
We currently have 40 features.
Too much for a user to enter.
"""

feature_importances = pd.DataFrame({'features': X.columns, 'importances': rand_search.best_estimator_.feature_importances_})

feature_importances.sort_values(by='importances', ascending=False)

"""We can see from above that the features that are most important to our random forest regressor are potential, value_eur, age, release_clause_eur and movement_reactions

We split our training and test data based on these features.
We are creating training and test data that only has these features
"""

features = ['potential', 'value_eur','wage_eur', 'age', 'release_clause_eur', 'movement_reactions']
Xtrain_sub = pd.concat([Xtrain[feature] for feature in features], axis = 1)
Xtest_sub = pd.concat([Xtest[feature] for feature in features], axis =1)
Xtrain_sub.head()

"""Creating a new model that based on what our random search cross validation found and training the model"""

new_model = rand_search = RandomizedSearchCV(forest_reg, parameters, scoring='neg_root_mean_squared_error', cv = 3, n_jobs=-1)
new_model.fit(Xtrain_sub, Ytrain)

"""Initial testing on our new model"""

new_model_pred= new_model.predict(Xtest_sub)

"""The model does quite well even though we've reduced data"""

print(f"""
Mean Absolute Error = {mean_absolute_error(new_model_pred, Ytest)},
Mean Squared Error = {mean_squared_error(new_model_pred, Ytest)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(new_model_pred, Ytest))},
R2 score = {r2_score(new_model_pred, Ytest)}
""")

"""Testing with the players 22 data

We need to create a version of the players 22 data that only has the key features
"""

new_model_pred_new_data = new_model.predict(pd.concat([new_test_X[feature] for feature in features], axis = 1))

"""The model is slightly worse, but its decent"""

print(f"""
Mean Absolute Error = {mean_absolute_error(new_model_pred_new_data, new_test_Y)},
Mean Squared Error = {mean_squared_error(new_model_pred_new_data, new_test_Y)},
Root Mean Squared Error = {np.sqrt(mean_squared_error(new_model_pred_new_data, new_test_Y))},
R2 score = {r2_score(new_model_pred_new_data, new_test_Y)}
""")

scaler = StandardScaler()
scaler.fit(df[features]) #fit the scaler on the reduced feature set from the legacy training set
model_pipeline = Pipeline([
    #('scaler', scaler),
    ('model', new_model)
])

players_22[features+['overall']].head(10222)

pred = model_pipeline.predict(pd.DataFrame([{
    'potential': 50,
    'value_eur': 1000000.00,
    'wage_eur':1000.00,
    'age':20,
    'release_clause_eur':10000000.00,
    'movement_reactions': 100
}]))
pred[0]

"""Saving the pipeline and the model"""

pkl.dump(model_pipeline, open('/content/model_pipeline.pkl', 'wb'))

